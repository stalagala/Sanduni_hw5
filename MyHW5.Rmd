---
title: "HW5"
author: "Sanduni Talagala"
date: "March 5, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/owner1/Desktop/bi612/Homework5/Sanduni HW5")
```

#1)
get the file
```{r}
crime<-read.csv("crimedata.csv")
```

##2) build regression tree
vriables: ExpenditureYear, StateSize, BelowWage, Education, YouthUnemployment,Youth
Note: Im not making a training dataset since I have another dataset to test on.
```{r}
#summary of dataset
summary(crime)

#Regression tree with whole dataset
wholecrime<-rpart(CrimeRate ~ ExpenditureYear+StateSize+BelowWage+Education+YouthUnemployment+Youth, data = crime)
```

##3) Print summary of tree

most important variables: ExpenditureYear was the most important variable, followed closely by Youth, then BelowWage, then StateSize, then YouthUnemployment and finally Education as the least important.
```{r}
#summary of tree using all the data
summary(wholecrime)
```

##4)Plot my tree and describe

Description tree: 
  The main and most important split is for yearly police expenditure and if it is greater than or less than 77. 51.1% of the data has early police expenditure above 77 and the mean crime rate in that group is 120. Then, of the ones with police expenditure above 77, if there is above 131 youths then the crime rate mean is 133 which inludes 27.7% of the data. Alternatively if there's less than 131 youths then the mean crime rate is 105 which includes 23.4% of the data. 
  When the police expenditure is less than 77 the most important next factor is the state size (not youth like when expenditure is above 77). The mean crime rate when expenditure is below 77 is 84.5 which includes 48.9% of the data. In that group, if the state size is above 23 hundred thousand, the mean crime rate is 97.6 which includes 23.4% of the data. Alternatively, if the state size is below 23 hundred thousand, the mean crime rate is 72.5 which includes 25.5% of the data.

```{r}
#install.packages("rpart.plot")
library(rpart.plot)

#Plot tree
rpart.plot(wholecrime, digits = 3, fallen.leaves = TRUE,tweak=1.3)
```

##5) Mean crime rate for each group

*Police expenditure above 77 - mean crime rate 120
*Police expenditure below 77 - mean crime rate 84.5
*State size above 23 (with police expenditure below 77) - mean crime rate is 97.6
*State size below 23 (with police expenditure below 77) - mean crime rate is 72.5
*Youth count above 131 (with police expenditure above 77) - mean crime rate is 133
*Youth count below 131 (with police expenditure above 77) - mean crime rate is 105

##6) Were variables excluded? Why?
This finds the first variable that splits the data into 2 (the best split that give the most homogeneous groups). Then once the data is seperated, this is again done to each seperate group (the 2 groups that were split). This repeats where each subgroup and split until the groups have minimul datapoints (depends on the dataset size) or until no more splits can be made (to make the tree any more homogeneous). They basically make splits until the cp(comlexity parameter) cannot get better any more. In our case, the best split was done using the expenditure of the year. Then depending on the first split, the next split was done based in state size or youth. After this no any more splits would be too small of a cp(not much improvement), so we do not want to add other variables which would make my tree less parsimonious.

##7) Predict 10 year crime with ALL the predictor variables available in crime10
```{r}
crime10<-read.csv("crimedata10.csv")

#model predict crime 
Myp.rpart <- predict(wholecrime, crime10) 
Myp.rpart
```

##8) correlation

correlation coefficient = 0.6175003
```{r}
cor(Myp.rpart, crime10[["CrimeRate"]],method="pearson")
```

##9) Calculate MAE, is your model good at predicting?
MAE = 24.32924
meaning: we are on average 24.32924 far off of predicting the actual thing. 24.32924 Does not seem that far off so I would say our model is good, but in the next question we will compare it with random guess and know exactly how good ours is.
```{r}
MyMAE <- function(actual, predicted)  {
  mean(abs(actual - predicted))
}

MyMAE(predicted = Myp.rpart,actual = crime10[["CrimeRate"]])
```


##10)Null distribution
```{r}
Myactual = crime10[["CrimeRate"]]

MyMAE2 <- function(data,indices)  {
  d<-data[indices]
  return(mean(abs(Myactual - d)))
}

library(boot)
Myguesses=boot(data=Myactual, statistic=MyMAE2, R=1000)

hist(Myguesses$t)
```

##11) random vs real.
Random MAE was 44.49198 (this changes every time) and the real MAE was 24.32924. My model did better than random chance it seems. Next question will say if it was significantly better or not.
```{r}
Mymean<- mean(Myguesses$t)
Mymean
```


##12) Is random and real significantly different?
They are significantly different, so my model was significantly better at predicting the crime10 data than at random.
```{r}
Myp.value=length(which((Myguesses$t<24.32924)==T))/1000
Myp.value
```

